---
title: 'Met Christopher Manning, Professor of Computer Science and Linguistics, Stanford University'
excerpt: "When I met Professor Christopher Manning in Melbourne!<br/><img src='/images/Chathura_with_Christopher_Manning.jpeg' width='300' alt='Chathura met Professor Christopher Manning in Melbourne, 2025'>"
date: 2025-05-07
permalink: /posts/2013/08/blog-post-2/
tags:
  - Seminars
---
Meaning and Intelligence in Language Models
======

About Christopher Manning
------
Christopher Manning is the inaugural Thomas M. Siebel Professor in Machine Learning in the Departments of Computer Science and Linguistics at Stanford University, an Associate Director at the Stanford Institute for Human-Centered Artificial Intelligence (HAI), and an Investing Partner at AIX Ventures. His research is on computers that can intelligently process, understand, and generate human language. Chris is the most-cited researcher within Natural Language Processing (NLP), with best paper awards at the ACL, Coling, EMNLP, and CHI conferences and two ACL Test of Time awards; a member of the U.S. National Academy of Engineering; and recipient of the 2024 IEEE John von Neumann Medal for his pioneering work on applying neural network or deep learning approaches to human language understanding, which led into modern Large Language Models and Generative AI. He founded the Stanford NLP group, has written widely used NLP textbooks, and teaches the popular NLP class CS224N, which is also available online. At AIX Ventures, he is involved with finding, deciding, and mentoring AI-native startups. He is also Australian.

----Copied from seminar introduction---
------
Language Models have been around for decades but have suddenly taken the world by storm. In a surprising third act for anyone doing NLP in the 70s, 80s, 90s, or 2000s, in much of the popular media, artificial intelligence is now synonymous with language models. In this talk, Christopher will take a look backward at where language models came from and why they were so slow to emerge, a look inward to give some thoughts on meaning, intelligence, and what language models understand and know, and a look at some recent work on steering language models to respond well to peopleâ€™s questions and commands. In the first part, Christopher will argue that material beyond language is not necessary to having meaning and understanding, but it is very useful in most cases, and that adaptability and learning are vital to intelligence, and so the current strategy of building from huge curated data will not truly get us there, even though LLMs have so many good uses. In the second part, Christopher will introduce Direct Preference Optimization (DPO), a recent way of learning to steer LLMs from human preference data without the complex iterative training of traditional reinforcement learning methods. DPO leverages a mapping between reward functions and optimal policies to show that a suitable constrained reward maximization problem can be optimized exactly with a single training step. This method has opened up steering LLMs to a broad array of smaller players and is also usable for other goals such as improving the factuality of models.

